---
layout: default
---

# Research motivation


Consider the setting in which we are just inundated by an ever-shifting torrent of raw data. I'm interested in understanding *how to organize, process, and summarize this stream of data on the fly* to make use of and learn on it. 

In many settings, there is ‘too much data’ in two ways: (1) the sheer volume of data being collected, and (2) the high-dimensionality of the data, which is to say too much irrelevant data. There is also ‘not enough data’ because the cost of processing and labeling the data may be prohibitive. And finally, many learning systems are unable to gracefully deal with data that keeps changing.


Taking up these demands of modern machine learning, I am interested in the design and analysis of algorithms that **extend fundamental tools of statistical estimation from their classical settings into settings with limited memory, non-stationarity, and/or interactivity**. And since these are the settings in which humans seem to learn quite naturally, I’m especially curious about whether we can create simple, neurally-plausible algorithms to achieve this.

<br>

*Areas:* algorithmic statistics, machine learning theory, unsupervised, interactive, continual learning, streaming algorithms