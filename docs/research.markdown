---
layout: default
---

# Research motivation

Consider the setting in which we are just inundated by an ever-shifting torrent of raw data. I'm interested in understanding *how to organize, process, and summarize this stream of data on the fly* to make use of and learn on it. 

In many settings, there is ‘too much data’ in two ways: (1) the sheer volume of data being collected, and (2) the high-dimensionality of the data, which is to say too much irrelevant data. There is also ‘not enough data’ because the cost of processing and labeling the data may be prohibitive. And finally, many learning systems are unable to deal with data that keeps changing in a graceful way.


Taking up the demands of modern machine learning, I am interested in the design and analysis of algorithms that **extend fundamental statistical tools from their classical settings into settings with limited memory, non-stationarity, and/or interactivity**. And since these are the settings in which humans seem to learn quite naturally, I’m especially curious about whether we can create simple, neurally-plausible algorithms to achieve this.

<br>

**Research interest.** My research especially arises out of problems in machine learning where (i) there is too much data, (ii) there is not enough data, (iii) the data keeps changing, and possibly any or all three at the same time. 

*Areas:* algorithmic statistics, machine learning theory, unsupervised, interactive, continual learning, streaming algorithms

